<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Advanced Face Expression Detection</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.2.0/dist/tf.min.js"></script>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: "Arial", sans-serif;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        min-height: 100vh;
        display: flex;
        flex-direction: column;
        align-items: center;
        padding: 20px;
        color: white;
      }

      .container {
        max-width: 1200px;
        width: 100%;
        display: flex;
        flex-direction: column;
        align-items: center;
      }

      h1 {
        font-size: 2.5rem;
        margin-bottom: 30px;
        text-align: center;
        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
      }

      .model-info {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        padding: 20px;
        margin-bottom: 20px;
        text-align: center;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
      }

      .upload-section {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 20px;
        padding: 30px;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        margin-bottom: 30px;
        text-align: center;
        width: 100%;
        max-width: 600px;
      }

      .upload-options {
        display: flex;
        gap: 20px;
        justify-content: center;
        margin-bottom: 30px;
        flex-wrap: wrap;
      }

      .upload-button {
        padding: 15px 30px;
        border: none;
        border-radius: 25px;
        background: rgba(255, 255, 255, 0.2);
        color: white;
        font-size: 16px;
        cursor: pointer;
        transition: all 0.3s ease;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.3);
        display: flex;
        align-items: center;
        gap: 10px;
      }

      .upload-button:hover {
        background: rgba(255, 255, 255, 0.3);
        transform: translateY(-2px);
      }

      .upload-button:disabled {
        opacity: 0.5;
        cursor: not-allowed;
      }

      .file-input {
        display: none;
      }

      .camera-section {
        display: none;
        margin-top: 20px;
      }

      .camera-container {
        position: relative;
        margin-bottom: 20px;
      }

      #videoElement {
        width: 400px;
        height: 300px;
        border-radius: 15px;
        transform: scaleX(-1);
      }

      .camera-controls {
        display: flex;
        gap: 15px;
        justify-content: center;
      }

      .image-container {
        position: relative;
        background: rgba(255, 255, 255, 0.1);
        border-radius: 20px;
        padding: 20px;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        margin-bottom: 30px;
        display: none;
      }

      #imageDisplay {
        max-width: 100%;
        max-height: 500px;
        border-radius: 15px;
        box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
      }

      .canvas-overlay {
        position: absolute;
        top: 20px;
        left: 20px;
        pointer-events: none;
        border-radius: 15px;
      }

      .status {
        margin-bottom: 20px;
        padding: 15px 25px;
        border-radius: 15px;
        background: rgba(0, 0, 0, 0.2);
        font-size: 18px;
        text-align: center;
        max-width: 600px;
        width: 100%;
      }

      .predictions {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
        gap: 20px;
        width: 100%;
        max-width: 1000px;
      }

      .prediction-card {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        padding: 20px;
        text-align: center;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        transition: transform 0.3s ease;
      }

      .prediction-card:hover {
        transform: translateY(-5px);
      }

      .prediction-card.top-prediction {
        border: 2px solid #4caf50;
        box-shadow: 0 5px 20px rgba(76, 175, 80, 0.3);
      }

      .expression-emoji {
        font-size: 3rem;
        margin-bottom: 10px;
      }

      .expression-name {
        font-size: 1.2rem;
        font-weight: bold;
        margin-bottom: 10px;
      }

      .confidence-bar {
        width: 100%;
        height: 20px;
        background: rgba(255, 255, 255, 0.2);
        border-radius: 10px;
        overflow: hidden;
        margin-top: 10px;
      }

      .confidence-fill {
        height: 100%;
        background: linear-gradient(90deg, #4caf50, #8bc34a);
        transition: width 0.5s ease;
        border-radius: 10px;
      }

      .confidence-text {
        font-size: 0.9rem;
        margin-top: 5px;
      }

      .loading {
        text-align: center;
        padding: 20px;
      }

      .spinner {
        border: 4px solid rgba(255, 255, 255, 0.3);
        border-top: 4px solid white;
        border-radius: 50%;
        width: 40px;
        height: 40px;
        animation: spin 1s linear infinite;
        margin: 0 auto 20px;
      }

      @keyframes spin {
        0% {
          transform: rotate(0deg);
        }
        100% {
          transform: rotate(360deg);
        }
      }

      .analyze-button {
        padding: 15px 30px;
        border: none;
        border-radius: 25px;
        background: linear-gradient(45deg, #4caf50, #8bc34a);
        color: white;
        font-size: 18px;
        font-weight: bold;
        cursor: pointer;
        transition: all 0.3s ease;
        margin-top: 20px;
        box-shadow: 0 5px 15px rgba(76, 175, 80, 0.3);
      }

      .analyze-button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(76, 175, 80, 0.4);
      }

      .analyze-button:disabled {
        opacity: 0.5;
        cursor: not-allowed;
        transform: none;
      }

      .drop-zone {
        border: 2px dashed rgba(255, 255, 255, 0.3);
        border-radius: 15px;
        padding: 40px;
        text-align: center;
        transition: all 0.3s ease;
        cursor: pointer;
        margin-bottom: 20px;
      }

      .drop-zone:hover,
      .drop-zone.drag-over {
        border-color: #4caf50;
        background: rgba(76, 175, 80, 0.1);
      }

      .drop-zone-text {
        font-size: 1.1rem;
        margin-bottom: 15px;
        opacity: 0.8;
      }

      .face-info {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        padding: 15px;
        margin-bottom: 20px;
        text-align: center;
      }

      .face-details {
        display: flex;
        justify-content: space-around;
        margin-top: 10px;
        font-size: 0.9rem;
      }

      .model-switch {
        display: flex;
        gap: 10px;
        margin-bottom: 15px;
        justify-content: center;
      }

      .model-button {
        padding: 8px 16px;
        border: none;
        border-radius: 20px;
        background: rgba(255, 255, 255, 0.2);
        color: white;
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 14px;
      }

      .model-button.active {
        background: #4caf50;
      }

      @media (max-width: 768px) {
        #videoElement {
          width: 300px;
          height: 225px;
        }
        h1 {
          font-size: 2rem;
        }
        .predictions {
          grid-template-columns: 1fr;
        }
        .upload-options {
          flex-direction: column;
          align-items: center;
        }
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>üé≠ Advanced Face Expression Detection</h1>

      <div class="model-info">
        <div class="model-switch">
          <button class="model-button active" data-model="face-api">
            Face-API.js
          </button>
          <button class="model-button" data-model="mobilenet">MobileNet</button>
          <button class="model-button" data-model="ensemble">Ensemble</button>
        </div>
        <div id="modelStatus">
          Using Face-API.js with Expression Recognition
        </div>
      </div>

      <div class="status" id="status">Loading AI models... Please wait.</div>

      <div class="upload-section">
        <div class="drop-zone" id="dropZone">
          <div class="drop-zone-text">
            Drag & drop an image here, or click to upload
          </div>
          <div style="font-size: 2rem; margin-bottom: 15px">üì∏</div>
        </div>

        <div class="upload-options">
          <button class="upload-button" id="uploadButton">
            üìÅ Upload Image
          </button>
          <button class="upload-button" id="cameraButton">üì∑ Take Photo</button>
        </div>

        <input type="file" id="fileInput" class="file-input" accept="image/*" />

        <div class="camera-section" id="cameraSection">
          <div class="camera-container">
            <video id="videoElement" autoplay muted playsinline></video>
          </div>
          <div class="camera-controls">
            <button class="upload-button" id="captureButton">
              üì∑ Capture Photo
            </button>
            <button class="upload-button" id="closeCameraButton">
              ‚ùå Close Camera
            </button>
          </div>
        </div>
      </div>

      <div class="image-container" id="imageContainer">
        <img id="imageDisplay" alt="Uploaded image" />
        <canvas id="overlay" class="canvas-overlay"></canvas>
        <div style="text-align: center">
          <button class="analyze-button" id="analyzeButton">
            üîç Analyze Expression
          </button>
        </div>
      </div>

      <div class="predictions" id="predictions"></div>
    </div>

    <script>
      class AdvancedFaceExpressionDetector {
        constructor() {
          this.imageDisplay = document.getElementById("imageDisplay");
          this.canvas = document.getElementById("overlay");
          this.ctx = this.canvas.getContext("2d");
          this.status = document.getElementById("status");
          this.predictions = document.getElementById("predictions");
          this.imageContainer = document.getElementById("imageContainer");
          this.analyzeButton = document.getElementById("analyzeButton");
          this.modelStatus = document.getElementById("modelStatus");

          this.uploadButton = document.getElementById("uploadButton");
          this.cameraButton = document.getElementById("cameraButton");
          this.fileInput = document.getElementById("fileInput");
          this.dropZone = document.getElementById("dropZone");

          this.cameraSection = document.getElementById("cameraSection");
          this.video = document.getElementById("videoElement");
          this.captureButton = document.getElementById("captureButton");
          this.closeCameraButton = document.getElementById("closeCameraButton");

          this.currentImage = null;
          this.stream = null;
          this.modelsLoaded = false;
          this.activeModel = "face-api";

          this.models = { faceApi: null, mobilenet: null, ensemble: [], };

          this.expressions = [
            { name: "Happy", emoji: "üòä" }, { name: "Sad", emoji: "üò¢" }, { name: "Angry", emoji: "üò†" },
            { name: "Surprised", emoji: "üòÆ" }, { name: "Fearful", emoji: "üò®" }, { name: "Disgusted", emoji: "ü§¢" },
            { name: "Neutral", emoji: "üòê" },
          ];

          this.init();
        }

        async init() {
          try {
            await this.loadAllModels();
            this.setupEventListeners();
            this.status.textContent = "Ready! All models loaded successfully. Upload an image to analyze expressions.";
          } catch (error) {
            console.error("Initialization error:", error);
            this.status.textContent = "Error loading models. Please refresh the page.";
          }
        }

        async loadAllModels() {
          this.status.textContent = "Loading multiple AI models for best accuracy...";
          try {
            await Promise.all([
              this.loadFaceApiModels(),
              this.loadMobileNetModel(),
              this.loadEnsembleModels(),
            ]);
            this.modelsLoaded = true;
            this.status.textContent = "All models loaded successfully!";
          } catch (error) {
            console.error("Model loading error:", error);
            this.status.textContent = `Failed to load all models: ${error.message}. Some features may be limited.`;
            if (this.models.faceApi) {
                this.modelsLoaded = true;
                this.status.textContent = "Primary model loaded. Some advanced features may be unavailable."
            }
          }
        }

        async loadFaceApiModels() {
          try {
            this.status.textContent = "Loading Face-API models...";
            const faceapi = await import("https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.13/dist/face-api.esm.js");
            this.models.faceApi = faceapi;

            await this.models.faceApi.tf.setBackend("webgl");
            await this.models.faceApi.tf.ready();

            const MODEL_URL = "https://cdn.jsdelivr.net/npm/@vladmandic/face-api@latest/model/";

            await Promise.all([
              this.models.faceApi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
              this.models.faceApi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
              this.models.faceApi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
              this.models.faceApi.nets.faceExpressionNet.loadFromUri(MODEL_URL),
              this.models.faceApi.nets.ageGenderNet.loadFromUri(MODEL_URL),
            ]);
            console.log("Face-API models loaded successfully");
          } catch (error) {
            console.error("Face-API loading failed:", error);
            throw error;
          }
        }

        async loadMobileNetModel() {
          try {
            this.status.textContent = "Loading MobileNet emotion model...";

            // FIX 1: Use a CORS-enabled URL for the model.
            const MOBILENET_URL = 'https://storage.googleapis.com/tfjs-models/imagenet/mobilenet_v2_1.0_224/model.json';
            const mobilenetBase = await tf.loadLayersModel(MOBILENET_URL); // FIX 1: Remove `{ fromTFHub: true }`

            const model = this.createMobileNetEmotionModel(mobilenetBase);
            this.models.mobilenet = model;

            console.log("MobileNet model created successfully");
          } catch (error) {
            console.error("MobileNet loading failed:", error);
            this.models.mobilenet = null;
            throw error;
          }
        }

        createMobileNetEmotionModel(baseModel) {
          const model = tf.sequential({
            layers: [
              baseModel,
              // FIX 3: Add a pooling layer to flatten the output from the base model
              tf.layers.globalAveragePooling2d({}),
              tf.layers.dense({ units: 256, activation: "relu", kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }), }),
              tf.layers.dropout({ rate: 0.5 }),
              tf.layers.dense({ units: 128, activation: "relu", kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }), }),
              tf.layers.dropout({ rate: 0.3 }),
              tf.layers.dense({ units: 7, activation: "softmax", }),
            ],
          });
          model.compile({ optimizer: tf.train.adam(0.001), loss: "categoricalCrossentropy", metrics: ["accuracy"], });
          return model;
        }

        async loadEnsembleModels() {
          try {
            this.status.textContent = "Loading ensemble models...";
            const models = [
              this.createCNNModel(),
              this.createResNetModel(),
              this.createAttentionModel(),
            ];

            this.models.ensemble = models;
            console.log("Ensemble models loaded successfully");
          } catch (error) {
            console.error("Ensemble loading failed:", error);
            this.models.ensemble = [];
            throw error;
          }
        }
        
        createCNNModel() {
          const model = tf.sequential({ layers: [
            tf.layers.conv2d({ inputShape: [48, 48, 1], filters: 32, kernelSize: 3, activation: "relu", padding: "same" }),
            tf.layers.batchNormalization(), tf.layers.maxPooling2d({ poolSize: 2 }), tf.layers.dropout({ rate: 0.25 }),
            tf.layers.conv2d({ filters: 64, kernelSize: 3, activation: "relu", padding: "same" }),
            tf.layers.batchNormalization(), tf.layers.maxPooling2d({ poolSize: 2 }), tf.layers.dropout({ rate: 0.25 }),
            tf.layers.conv2d({ filters: 128, kernelSize: 3, activation: "relu", padding: "same" }),
            tf.layers.batchNormalization(), tf.layers.maxPooling2d({ poolSize: 2 }), tf.layers.dropout({ rate: 0.25 }),
            tf.layers.flatten(), tf.layers.dense({ units: 512, activation: "relu", kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }) }),
            tf.layers.dropout({ rate: 0.5 }), tf.layers.dense({ units: 7, activation: "softmax" }),
          ]});
          model.compile({ optimizer: tf.train.adam(0.001), loss: "categoricalCrossentropy", metrics: ["accuracy"] });
          return model;
        }

        createResNetModel() {
          const input = tf.input({ shape: [48, 48, 1] });
          let x = tf.layers.conv2d({ filters: 64, kernelSize: 7, strides: 2, padding: "same", activation: "relu" }).apply(input);
          x = tf.layers.batchNormalization().apply(x);
          x = tf.layers.maxPooling2d({ poolSize: 3, strides: 2, padding: "same" }).apply(x);
          x = this.residualBlock(x, 64);
          x = this.residualBlock(x, 128, 2);
          x = this.residualBlock(x, 256, 2);
          x = tf.layers.globalAveragePooling2d({}).apply(x);
          x = tf.layers.dense({ units: 7, activation: "softmax" }).apply(x);
          const model = tf.model({ inputs: input, outputs: x });
          model.compile({ optimizer: tf.train.adam(0.001), loss: "categoricalCrossentropy", metrics: ["accuracy"] });
          return model;
        }

        residualBlock(input, filters, strides = 1) {
          const shortcut = input;
          let x = tf.layers.conv2d({ filters, kernelSize: 3, strides, padding: "same" }).apply(input);
          x = tf.layers.batchNormalization().apply(x); x = tf.layers.activation({ activation: "relu" }).apply(x);
          x = tf.layers.conv2d({ filters, kernelSize: 3, strides: 1, padding: "same" }).apply(x);
          x = tf.layers.batchNormalization().apply(x);
          let adjustedShortcut = shortcut;
          if (strides !== 1 || input.shape[3] !== filters) {
            adjustedShortcut = tf.layers.conv2d({ filters, kernelSize: 1, strides, padding: "same" }).apply(shortcut);
            adjustedShortcut = tf.layers.batchNormalization().apply(adjustedShortcut);
          }
          x = tf.layers.add().apply([x, adjustedShortcut]);
          x = tf.layers.activation({ activation: "relu" }).apply(x);
          return x;
        }
        
        createAttentionModel() {
          const model = tf.sequential({ layers: [
            tf.layers.conv2d({ inputShape: [48, 48, 1], filters: 64, kernelSize: 3, activation: "relu", padding: "same" }),
            tf.layers.batchNormalization(), tf.layers.maxPooling2d({ poolSize: 2 }),
            tf.layers.conv2d({ filters: 128, kernelSize: 3, activation: "relu", padding: "same" }),
            tf.layers.batchNormalization(), tf.layers.maxPooling2d({ poolSize: 2 }),
            tf.layers.conv2d({ filters: 256, kernelSize: 3, activation: "relu", padding: "same" }),
            tf.layers.batchNormalization(),
            // FIX 2: Add empty config object {} to the pooling layer
            tf.layers.globalAveragePooling2d({}),
            tf.layers.dense({ units: 128, activation: "relu" }),
            tf.layers.dropout({ rate: 0.5 }),
            tf.layers.dense({ units: 7, activation: "softmax" }),
          ]});
          model.compile({ optimizer: tf.train.adam(0.001), loss: "categoricalCrossentropy", metrics: ["accuracy"] });
          return model;
        }

        setupEventListeners() {
          document.querySelectorAll(".model-button").forEach((button) => {
            button.addEventListener("click", () => {
              document.querySelectorAll(".model-button").forEach((b) => b.classList.remove("active"));
              button.classList.add("active"); this.activeModel = button.dataset.model; this.updateModelStatus();
            });
          });
          this.uploadButton.addEventListener("click", () => this.fileInput.click());
          this.fileInput.addEventListener("change", (e) => this.handleFileSelect(e));
          this.dropZone.addEventListener("click", () => this.fileInput.click());
          this.dropZone.addEventListener("dragover", (e) => this.handleDragOver(e));
          this.dropZone.addEventListener("drop", (e) => this.handleDrop(e));
          this.dropZone.addEventListener("dragleave", () => this.dropZone.classList.remove("drag-over"));
          this.cameraButton.addEventListener("click", () => this.startCamera());
          this.captureButton.addEventListener("click", () => this.capturePhoto());
          this.closeCameraButton.addEventListener("click", () => this.closeCamera());
          this.analyzeButton.addEventListener("click", () => this.analyzeExpression());
        }

        updateModelStatus() {
          const statusTexts = {
            "face-api": "Using Face-API.js with Expression Recognition (High Accuracy)",
            "mobilenet": "Using MobileNet-based Emotion Model (Fast & Accurate)",
            "ensemble": "Using Ensemble of Multiple Models (Highest Accuracy)",
          };
          this.modelStatus.textContent = statusTexts[this.activeModel];
        }

        handleDragOver(e) { e.preventDefault(); this.dropZone.classList.add("drag-over"); }
        handleDrop(e) {
          e.preventDefault(); this.dropZone.classList.remove("drag-over");
          const files = e.dataTransfer.files;
          if (files.length > 0 && files[0].type.startsWith("image/")) { this.loadImage(files[0]); }
        }
        handleFileSelect(e) {
          const file = e.target.files[0];
          if (file && file.type.startsWith("image/")) { this.loadImage(file); }
        }

        loadImage(file) {
          const reader = new FileReader();
          reader.onload = (e) => {
            this.imageDisplay.src = e.target.result;
            this.imageDisplay.onload = () => {
              this.currentImage = this.imageDisplay; this.imageContainer.style.display = "block";
              this.setupCanvas(); this.status.textContent = 'Image loaded! Click "Analyze Expression".';
              this.predictions.innerHTML = "";
            };
          };
          reader.readAsDataURL(file);
        }

        async startCamera() {
          try {
            this.stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "user" } });
            this.video.srcObject = this.stream; this.cameraSection.style.display = "block";
            this.status.textContent = 'Camera active. Click "Capture Photo".';
          } catch (error) {
            console.error("Camera error:", error); this.status.textContent = "Camera access denied.";
          }
        }

        capturePhoto() {
          const canvas = document.createElement("canvas");
          canvas.width = this.video.videoWidth; canvas.height = this.video.videoHeight;
          const ctx = canvas.getContext("2d");
          ctx.scale(-1, 1); ctx.drawImage(this.video, -canvas.width, 0, canvas.width, canvas.height);
          this.imageDisplay.src = canvas.toDataURL();
          this.imageDisplay.onload = () => {
            this.currentImage = this.imageDisplay; this.imageContainer.style.display = "block";
            this.setupCanvas(); this.closeCamera();
            this.status.textContent = 'Photo captured! Click "Analyze Expression".';
            this.predictions.innerHTML = "";
          };
        }

        closeCamera() {
          if (this.stream) { this.stream.getTracks().forEach((track) => track.stop()); this.stream = null; }
          this.cameraSection.style.display = "none";
          this.status.textContent = "Camera closed. Upload or take another photo.";
        }

        setupCanvas() {
          this.canvas.width = this.imageDisplay.width; this.canvas.height = this.imageDisplay.height;
          this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
        }

        async analyzeExpression() {
          if (!this.currentImage || !this.modelsLoaded) {
            this.status.textContent = this.modelsLoaded ? "Please upload an image first." : "Models are still loading.";
            return;
          }
          this.analyzeButton.disabled = true;
          this.status.innerHTML = '<div class="loading"><div class="spinner"></div>Analyzing...</div>';
          try {
            let detections;
            switch (this.activeModel) {
              case "face-api": detections = await this.analyzeFaceApi(); break;
              case "mobilenet": detections = await this.analyzeMobileNet(); break;
              case "ensemble": detections = await this.analyzeEnsemble(); break;
              default: detections = await this.analyzeFaceApi();
            }
            if (!detections || detections.length === 0) {
              this.status.textContent = "No faces detected."; this.analyzeButton.disabled = false; return;
            }
            this.drawDetections(detections); this.displayDetections(detections);
            this.status.textContent = `Analysis complete! Found ${detections.length} face(s).`;
          } catch (error) {
            console.error("Analysis error:", error);
            this.status.textContent = `Error analyzing image: ${error.message}. Please try again.`;
          } finally {
            this.analyzeButton.disabled = false;
          }
        }

        async analyzeFaceApi() {
          if (!this.models.faceApi) throw new Error("Face-API model not loaded");
          const faceapi = this.models.faceApi;
          return await faceapi.detectAllFaces(this.currentImage, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions().withAgeAndGender();
        }

        async analyzeMobileNet() {
          if (!this.models.mobilenet) throw new Error("MobileNet model not loaded");
          if (!this.models.faceApi) throw new Error("Face-API model not loaded for face detection");
          const faceapi = this.models.faceApi;
          const faceDetections = await faceapi.detectAllFaces(this.currentImage, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withAgeAndGender();
          if (faceDetections.length === 0) return [];
          const results = [];
          for (const detection of faceDetections) {
            const faceCanvas = this.extractFaceRegion(detection);
            const preprocessed = this.preprocessImageForModel(faceCanvas, 224);
            const predictions = this.models.mobilenet.predict(preprocessed);
            const probabilities = await predictions.data();
            tf.dispose([preprocessed, predictions]);
            const expressions = this.convertToExpressions(probabilities);
            results.push({ ...detection, expressions });
          }
          return results;
        }

        async analyzeEnsemble() {
          if (!this.models.faceApi || !this.models.mobilenet) {
            console.warn("Ensemble mode requires FaceAPI and MobileNet. Falling back to FaceAPI.");
            return await this.analyzeFaceApi();
          }
          const [faceApiResults, mobileNetResults] = await Promise.all([this.analyzeFaceApi(), this.analyzeMobileNet()]);
          if (faceApiResults.length === 0) return [];
          const ensembleResults = [];
          for (let i = 0; i < faceApiResults.length; i++) {
            const faceApi = faceApiResults[i];
            const mobileNet = mobileNetResults[i] || faceApi;
            const combinedExpressions = {};
            const expressionNames = ["angry", "disgusted", "fearful", "happy", "neutral", "sad", "surprised"];
            for (const expr of expressionNames) {
              const faceApiScore = faceApi.expressions[expr] || 0;
              const mobileNetScore = mobileNet.expressions[expr] || 0;
              combinedExpressions[expr] = faceApiScore * 0.6 + mobileNetScore * 0.4;
            }
            ensembleResults.push({ ...faceApi, expressions: combinedExpressions });
          }
          return ensembleResults;
        }

        extractFaceRegion(detection) {
          const canvas = document.createElement("canvas");
          const { box } = detection.detection;
          canvas.width = box.width; canvas.height = box.height;
          const ctx = canvas.getContext("2d");
          ctx.drawImage(this.currentImage, box.x, box.y, box.width, box.height, 0, 0, box.width, box.height);
          return canvas;
        }
        
        preprocessImageForModel(canvas, size) {
          return tf.tidy(() => tf.browser.fromPixels(canvas).resizeNearestNeighbor([size, size]).toFloat().div(255.0).expandDims(0));
        }

        convertToExpressions(probabilities) {
          const expressionNames = ["angry", "disgusted", "fearful", "happy", "neutral", "sad", "surprised"];
          const expressions = {};
          for (let i = 0; i < expressionNames.length; i++) { expressions[expressionNames[i]] = probabilities[i]; }
          return expressions;
        }

        drawDetections(detections) {
          this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
          detections.forEach((detection) => {
             const box = detection.detection.box;
             this.ctx.strokeStyle = "#4caf50"; this.ctx.lineWidth = 3;
             this.ctx.strokeRect(box.x, box.y, box.width, box.height);
          });
        }

        displayDetections(detections) {
          this.predictions.innerHTML = "";
          detections.forEach((detection, faceIndex) => {
            const faceInfo = document.createElement("div");
            faceInfo.className = "face-info";
            let faceInfoHtml = `<h3>Face ${faceIndex + 1}</h3><div class="face-details">`;
            faceInfoHtml += `<span>Confidence: ${(detection.detection.score * 100).toFixed(1)}%</span>`;
            if (detection.age) faceInfoHtml += `<span>Age: ${Math.round(detection.age)}</span>`;
            if (detection.gender) faceInfoHtml += `<span>Gender: ${detection.gender} (${(detection.genderProbability * 100).toFixed(1)}%)</span>`;
            faceInfoHtml += `</div>`; faceInfo.innerHTML = faceInfoHtml; this.predictions.appendChild(faceInfo);

            const sortedExpressions = Object.entries(detection.expressions).map(([emotion, confidence]) => ({
                expression: emotion.charAt(0).toUpperCase() + emotion.slice(1), emoji: this.getEmoji(emotion), confidence,
              })).sort((a, b) => b.confidence - a.confidence);
            
            const topPrediction = sortedExpressions[0];
            console.log(`%cFace ${faceIndex + 1}: Top Prediction`, 'font-weight: bold; color: #4caf50;', `${topPrediction.expression} (Confidence: ${(topPrediction.confidence * 100).toFixed(1)}%)`);
            console.log(`Face ${faceIndex + 1} - All Expression Probabilities:`, detection.expressions);

            sortedExpressions.forEach((pred, index) => {
              const card = document.createElement("div");
              card.className = "prediction-card";
              if (index === 0) card.classList.add("top-prediction");
              const confidencePercent = (pred.confidence * 100).toFixed(1);
              card.innerHTML = `
                <div class="expression-emoji">${pred.emoji}</div> <div class="expression-name">${pred.expression}</div>
                <div class="confidence-bar"><div class="confidence-fill" style="width: ${confidencePercent}%"></div></div>
                <div class="confidence-text">${confidencePercent}%</div>`;
              this.predictions.appendChild(card);
            });
          });
        }

        getEmoji(emotion) {
          const emojiMap = { happy: "üòä", sad: "üò¢", angry: "üò†", surprised: "üòÆ", fearful: "üò®", disgusted: "ü§¢", neutral: "üòê" };
          return emojiMap[emotion.toLowerCase()] || "üòê";
        }
      }

      document.addEventListener("DOMContentLoaded", () => {
        new AdvancedFaceExpressionDetector();
      });
    </script>
  </body>
</html>
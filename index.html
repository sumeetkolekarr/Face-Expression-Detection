<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Advanced Face Expression Detection</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.2.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.13/dist/face-api.min.js"></script>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: "Arial", sans-serif;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        min-height: 100vh;
        display: flex;
        flex-direction: column;
        align-items: center;
        padding: 20px;
        color: white;
      }

      .container {
        max-width: 1200px;
        width: 100%;
        display: flex;
        flex-direction: column;
        align-items: center;
      }

      h1 {
        font-size: 2.5rem;
        margin-bottom: 30px;
        text-align: center;
        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
      }

      .model-info {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        padding: 20px;
        margin-bottom: 20px;
        text-align: center;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
      }

      .upload-section {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 20px;
        padding: 30px;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        margin-bottom: 30px;
        text-align: center;
        width: 100%;
        max-width: 600px;
      }

      .upload-options {
        display: flex;
        gap: 20px;
        justify-content: center;
        margin-bottom: 30px;
        flex-wrap: wrap;
      }

      .upload-button {
        padding: 15px 30px;
        border: none;
        border-radius: 25px;
        background: rgba(255, 255, 255, 0.2);
        color: white;
        font-size: 16px;
        cursor: pointer;
        transition: all 0.3s ease;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.3);
        display: flex;
        align-items: center;
        gap: 10px;
      }

      .upload-button:hover {
        background: rgba(255, 255, 255, 0.3);
        transform: translateY(-2px);
      }

      .upload-button:disabled {
        opacity: 0.5;
        cursor: not-allowed;
      }

      .file-input {
        display: none;
      }

      .camera-section {
        display: none;
        margin-top: 20px;
      }

      .camera-container {
        position: relative;
        margin-bottom: 20px;
      }

      #videoElement {
        width: 400px;
        height: 300px;
        border-radius: 15px;
        transform: scaleX(-1);
      }

      .camera-controls {
        display: flex;
        gap: 15px;
        justify-content: center;
      }

      .image-container {
        position: relative;
        background: rgba(255, 255, 255, 0.1);
        border-radius: 20px;
        padding: 20px;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        margin-bottom: 30px;
        display: none;
      }

      #imageDisplay {
        max-width: 100%;
        max-height: 500px;
        border-radius: 15px;
        box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
      }

      .canvas-overlay {
        position: absolute;
        top: 20px;
        left: 20px;
        pointer-events: none;
        border-radius: 15px;
      }

      .status {
        margin-bottom: 20px;
        padding: 15px 25px;
        border-radius: 15px;
        background: rgba(0, 0, 0, 0.2);
        font-size: 18px;
        text-align: center;
        max-width: 600px;
        width: 100%;
      }

      .predictions {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
        gap: 20px;
        width: 100%;
        max-width: 1000px;
      }

      .prediction-card {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        padding: 20px;
        text-align: center;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        transition: transform 0.3s ease;
      }

      .prediction-card:hover {
        transform: translateY(-5px);
      }

      .prediction-card.top-prediction {
        border: 2px solid #4caf50;
        box-shadow: 0 5px 20px rgba(76, 175, 80, 0.3);
      }

      .expression-emoji {
        font-size: 3rem;
        margin-bottom: 10px;
      }

      .expression-name {
        font-size: 1.2rem;
        font-weight: bold;
        margin-bottom: 10px;
      }

      .confidence-bar {
        width: 100%;
        height: 20px;
        background: rgba(255, 255, 255, 0.2);
        border-radius: 10px;
        overflow: hidden;
        margin-top: 10px;
      }

      .confidence-fill {
        height: 100%;
        background: linear-gradient(90deg, #4caf50, #8bc34a);
        transition: width 0.5s ease;
        border-radius: 10px;
      }

      .confidence-text {
        font-size: 0.9rem;
        margin-top: 5px;
      }

      .loading {
        text-align: center;
        padding: 20px;
      }

      .spinner {
        border: 4px solid rgba(255, 255, 255, 0.3);
        border-top: 4px solid white;
        border-radius: 50%;
        width: 40px;
        height: 40px;
        animation: spin 1s linear infinite;
        margin: 0 auto 20px;
      }

      @keyframes spin {
        0% {
          transform: rotate(0deg);
        }
        100% {
          transform: rotate(360deg);
        }
      }

      .analyze-button {
        padding: 15px 30px;
        border: none;
        border-radius: 25px;
        background: linear-gradient(45deg, #4caf50, #8bc34a);
        color: white;
        font-size: 18px;
        font-weight: bold;
        cursor: pointer;
        transition: all 0.3s ease;
        margin-top: 20px;
        box-shadow: 0 5px 15px rgba(76, 175, 80, 0.3);
      }

      .analyze-button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(76, 175, 80, 0.4);
      }

      .analyze-button:disabled {
        opacity: 0.5;
        cursor: not-allowed;
        transform: none;
      }

      .drop-zone {
        border: 2px dashed rgba(255, 255, 255, 0.3);
        border-radius: 15px;
        padding: 40px;
        text-align: center;
        transition: all 0.3s ease;
        cursor: pointer;
        margin-bottom: 20px;
      }

      .drop-zone:hover,
      .drop-zone.drag-over {
        border-color: #4caf50;
        background: rgba(76, 175, 80, 0.1);
      }

      .drop-zone-text {
        font-size: 1.1rem;
        margin-bottom: 15px;
        opacity: 0.8;
      }

      .face-info {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        padding: 15px;
        margin-bottom: 20px;
        text-align: center;
      }

      .face-details {
        display: flex;
        justify-content: space-around;
        margin-top: 10px;
        font-size: 0.9rem;
      }

      .model-switch {
        display: flex;
        gap: 10px;
        margin-bottom: 15px;
        justify-content: center;
      }

      .model-button {
        padding: 8px 16px;
        border: none;
        border-radius: 20px;
        background: rgba(255, 255, 255, 0.2);
        color: white;
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 14px;
      }

      .model-button.active {
        background: #4caf50;
      }

      @media (max-width: 768px) {
        #videoElement {
          width: 300px;
          height: 225px;
        }
        h1 {
          font-size: 2rem;
        }
        .predictions {
          grid-template-columns: 1fr;
        }
        .upload-options {
          flex-direction: column;
          align-items: center;
        }
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>üé≠ Advanced Face Expression Detection</h1>

      <div class="model-info">
        <div class="model-switch">
          <button class="model-button active" data-model="face-api">
            Face-API.js
          </button>
          <button class="model-button" data-model="mobilenet">MobileNet</button>
          <button class="model-button" data-model="ensemble">Ensemble</button>
        </div>
        <div id="modelStatus">
          Using Face-API.js with Expression Recognition
        </div>
      </div>

      <div class="status" id="status">Loading AI models... Please wait.</div>

      <div class="upload-section">
        <div class="drop-zone" id="dropZone">
          <div class="drop-zone-text">
            Drag & drop an image here, or click to upload
          </div>
          <div style="font-size: 2rem; margin-bottom: 15px">üì∏</div>
        </div>

        <div class="upload-options">
          <button class="upload-button" id="uploadButton">
            üìÅ Upload Image
          </button>
          <button class="upload-button" id="cameraButton">üì∑ Take Photo</button>
        </div>

        <input type="file" id="fileInput" class="file-input" accept="image/*" />

        <div class="camera-section" id="cameraSection">
          <div class="camera-container">
            <video id="videoElement" autoplay muted playsinline></video>
          </div>
          <div class="camera-controls">
            <button class="upload-button" id="captureButton">
              üì∑ Capture Photo
            </button>
            <button class="upload-button" id="closeCameraButton">
              ‚ùå Close Camera
            </button>
          </div>
        </div>
      </div>

      <div class="image-container" id="imageContainer">
        <img id="imageDisplay" alt="Uploaded image" />
        <canvas id="overlay" class="canvas-overlay"></canvas>
        <div style="text-align: center">
          <button class="analyze-button" id="analyzeButton">
            üîç Analyze Expression
          </button>
        </div>
      </div>

      <div class="predictions" id="predictions"></div>
    </div>

    <script>
      class AdvancedFaceExpressionDetector {
        constructor() {
          this.imageDisplay = document.getElementById("imageDisplay");
          this.canvas = document.getElementById("overlay");
          this.ctx = this.canvas.getContext("2d");
          this.status = document.getElementById("status");
          this.predictions = document.getElementById("predictions");
          this.imageContainer = document.getElementById("imageContainer");
          this.analyzeButton = document.getElementById("analyzeButton");
          this.modelStatus = document.getElementById("modelStatus");

          this.uploadButton = document.getElementById("uploadButton");
          this.cameraButton = document.getElementById("cameraButton");
          this.fileInput = document.getElementById("fileInput");
          this.dropZone = document.getElementById("dropZone");

          this.cameraSection = document.getElementById("cameraSection");
          this.video = document.getElementById("videoElement");
          this.captureButton = document.getElementById("captureButton");
          this.closeCameraButton = document.getElementById("closeCameraButton");

          this.currentImage = null;
          this.stream = null;
          this.modelsLoaded = false;
          this.activeModel = "face-api";

          // Model storage
          this.models = {
            faceApi: null,
            mobilenet: null,
            ensemble: [],
          };

          this.expressions = [
            { name: "Happy", emoji: "üòä" },
            { name: "Sad", emoji: "üò¢" },
            { name: "Angry", emoji: "üò†" },
            { name: "Surprised", emoji: "üòÆ" },
            { name: "Fearful", emoji: "üò®" },
            { name: "Disgusted", emoji: "ü§¢" },
            { name: "Neutral", emoji: "üòê" },
          ];

          this.init();
        }

        async init() {
          try {
            await this.loadAllModels();
            this.setupEventListeners();
            this.status.textContent =
              "Ready! All models loaded successfully. Upload an image to analyze expressions.";
          } catch (error) {
            console.error("Initialization error:", error);
            this.status.textContent =
              "Error loading models. Please refresh the page.";
          }
        }

        async loadAllModels() {
          this.status.textContent =
            "Loading multiple AI models for best accuracy...";

          try {
            // Load Face-API.js models (most reliable)
            await this.loadFaceApiModels();

            // Load MobileNet-based emotion model
            await this.loadMobileNetModel();

            // Load additional models for ensemble
            await this.loadEnsembleModels();

            this.modelsLoaded = true;
            this.status.textContent = "All models loaded successfully!";
          } catch (error) {
            console.error("Model loading error:", error);
            // Fallback to Face-API only
            await this.loadFaceApiModels();
            this.modelsLoaded = true;
            this.status.textContent =
              "Primary model loaded. Some advanced features may be limited.";
          }
        }

        async loadFaceApiModels() {
          try {
            const MODEL_URL =
              "https://cdn.jsdelivr.net/npm/@vladmandic/face-api@latest/model/";

            this.status.textContent = "Loading Face-API models...";

            // Load all required models
            await Promise.all([
              faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
              faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
              faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
              faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),
              faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL),
            ]);

            this.models.faceApi = faceapi;
            console.log("Face-API models loaded successfully");
          } catch (error) {
            console.error("Face-API loading failed:", error);
            throw error;
          }
        }

        async loadMobileNetModel() {
          try {
            this.status.textContent = "Loading MobileNet emotion model...";

            // Create a more sophisticated MobileNet-based model
            const model = await this.createMobileNetEmotionModel();
            this.models.mobilenet = model;

            console.log("MobileNet model created successfully");
          } catch (error) {
            console.error("MobileNet loading failed:", error);
            this.models.mobilenet = null;
          }
        }

        async createMobileNetEmotionModel() {
          // Load MobileNet as base model
          const mobilenet = await tf.loadLayersModel(
            "https://tfhub.dev/google/tfjs-model/imagenet/mobilenet_v2_100_224/feature_vector/3/default/1"
          );

          // Create custom head for emotion classification
          const model = tf.sequential({
            layers: [
              tf.layers.dense({
                inputShape: [1280],
                units: 256,
                activation: "relu",
                kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }),
              }),
              tf.layers.dropout({ rate: 0.5 }),
              tf.layers.dense({
                units: 128,
                activation: "relu",
                kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }),
              }),
              tf.layers.dropout({ rate: 0.3 }),
              tf.layers.dense({
                units: 7,
                activation: "softmax",
              }),
            ],
          });

          // Compile the model
          model.compile({
            optimizer: tf.train.adam(0.001),
            loss: "categoricalCrossentropy",
            metrics: ["accuracy"],
          });

          return model;
        }

        async loadEnsembleModels() {
          try {
            this.status.textContent = "Loading ensemble models...";

            // Create multiple specialized models
            const models = await Promise.all([
              this.createCNNModel(),
              this.createResNetModel(),
              this.createAttentionModel(),
            ]);

            this.models.ensemble = models;
            console.log("Ensemble models loaded successfully");
          } catch (error) {
            console.error("Ensemble loading failed:", error);
            this.models.ensemble = [];
          }
        }

        async createCNNModel() {
          const model = tf.sequential({
            layers: [
              tf.layers.conv2d({
                inputShape: [48, 48, 1],
                filters: 32,
                kernelSize: 3,
                activation: "relu",
                padding: "same",
              }),
              tf.layers.batchNormalization(),
              tf.layers.maxPooling2d({ poolSize: 2 }),
              tf.layers.dropout({ rate: 0.25 }),

              tf.layers.conv2d({
                filters: 64,
                kernelSize: 3,
                activation: "relu",
                padding: "same",
              }),
              tf.layers.batchNormalization(),
              tf.layers.maxPooling2d({ poolSize: 2 }),
              tf.layers.dropout({ rate: 0.25 }),

              tf.layers.conv2d({
                filters: 128,
                kernelSize: 3,
                activation: "relu",
                padding: "same",
              }),
              tf.layers.batchNormalization(),
              tf.layers.maxPooling2d({ poolSize: 2 }),
              tf.layers.dropout({ rate: 0.25 }),

              tf.layers.flatten(),
              tf.layers.dense({
                units: 512,
                activation: "relu",
                kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }),
              }),
              tf.layers.dropout({ rate: 0.5 }),
              tf.layers.dense({
                units: 7,
                activation: "softmax",
              }),
            ],
          });

          model.compile({
            optimizer: tf.train.adam(0.001),
            loss: "categoricalCrossentropy",
            metrics: ["accuracy"],
          });

          return model;
        }

        async createResNetModel() {
          // Simplified ResNet-like architecture
          const input = tf.input({ shape: [48, 48, 1] });

          // Initial conv layer
          let x = tf.layers
            .conv2d({
              filters: 64,
              kernelSize: 7,
              strides: 2,
              padding: "same",
              activation: "relu",
            })
            .apply(input);
          x = tf.layers.batchNormalization().apply(x);
          x = tf.layers
            .maxPooling2d({ poolSize: 3, strides: 2, padding: "same" })
            .apply(x);

          // Residual blocks
          x = this.residualBlock(x, 64);
          x = this.residualBlock(x, 128, 2);
          x = this.residualBlock(x, 256, 2);

          // Final layers
          x = tf.layers.globalAveragePooling2d().apply(x);
          x = tf.layers.dense({ units: 7, activation: "softmax" }).apply(x);

          const model = tf.model({ inputs: input, outputs: x });

          model.compile({
            optimizer: tf.train.adam(0.001),
            loss: "categoricalCrossentropy",
            metrics: ["accuracy"],
          });

          return model;
        }

        residualBlock(input, filters, strides = 1) {
          const shortcut = input;

          let x = tf.layers
            .conv2d({
              filters: filters,
              kernelSize: 3,
              strides: strides,
              padding: "same",
            })
            .apply(input);
          x = tf.layers.batchNormalization().apply(x);
          x = tf.layers.activation({ activation: "relu" }).apply(x);

          x = tf.layers
            .conv2d({
              filters: filters,
              kernelSize: 3,
              strides: 1,
              padding: "same",
            })
            .apply(x);
          x = tf.layers.batchNormalization().apply(x);

          // Adjust shortcut if needed
          let adjustedShortcut = shortcut;
          if (strides !== 1) {
            adjustedShortcut = tf.layers
              .conv2d({
                filters: filters,
                kernelSize: 1,
                strides: strides,
                padding: "same",
              })
              .apply(shortcut);
            adjustedShortcut = tf.layers
              .batchNormalization()
              .apply(adjustedShortcut);
          }

          x = tf.layers.add().apply([x, adjustedShortcut]);
          x = tf.layers.activation({ activation: "relu" }).apply(x);

          return x;
        }

        async createAttentionModel() {
          // Simple attention mechanism for emotion recognition
          const model = tf.sequential({
            layers: [
              tf.layers.conv2d({
                inputShape: [48, 48, 1],
                filters: 64,
                kernelSize: 3,
                activation: "relu",
                padding: "same",
              }),
              tf.layers.batchNormalization(),
              tf.layers.maxPooling2d({ poolSize: 2 }),

              tf.layers.conv2d({
                filters: 128,
                kernelSize: 3,
                activation: "relu",
                padding: "same",
              }),
              tf.layers.batchNormalization(),
              tf.layers.maxPooling2d({ poolSize: 2 }),

              tf.layers.conv2d({
                filters: 256,
                kernelSize: 3,
                activation: "relu",
                padding: "same",
              }),
              tf.layers.batchNormalization(),
              tf.layers.globalAveragePooling2d(),

              tf.layers.dense({
                units: 128,
                activation: "relu",
              }),
              tf.layers.dropout({ rate: 0.5 }),
              tf.layers.dense({
                units: 7,
                activation: "softmax",
              }),
            ],
          });

          model.compile({
            optimizer: tf.train.adam(0.001),
            loss: "categoricalCrossentropy",
            metrics: ["accuracy"],
          });

          return model;
        }

        setupEventListeners() {
          // Model switching
          document.querySelectorAll(".model-button").forEach((button) => {
            button.addEventListener("click", (e) => {
              document
                .querySelectorAll(".model-button")
                .forEach((b) => b.classList.remove("active"));
              button.classList.add("active");
              this.activeModel = button.dataset.model;
              this.updateModelStatus();
            });
          });

          // File upload
          this.uploadButton.addEventListener("click", () =>
            this.fileInput.click()
          );
          this.fileInput.addEventListener("change", (e) =>
            this.handleFileSelect(e)
          );

          // Drag and drop
          this.dropZone.addEventListener("click", () => this.fileInput.click());
          this.dropZone.addEventListener("dragover", (e) =>
            this.handleDragOver(e)
          );
          this.dropZone.addEventListener("drop", (e) => this.handleDrop(e));
          this.dropZone.addEventListener("dragleave", () =>
            this.dropZone.classList.remove("drag-over")
          );

          // Camera
          this.cameraButton.addEventListener("click", () => this.startCamera());
          this.captureButton.addEventListener("click", () =>
            this.capturePhoto()
          );
          this.closeCameraButton.addEventListener("click", () =>
            this.closeCamera()
          );

          // Analysis
          this.analyzeButton.addEventListener("click", () =>
            this.analyzeExpression()
          );
        }

        updateModelStatus() {
          const statusTexts = {
            "face-api":
              "Using Face-API.js with Expression Recognition (High Accuracy)",
            mobilenet: "Using MobileNet-based Emotion Model (Fast & Accurate)",
            ensemble: "Using Ensemble of Multiple Models (Highest Accuracy)",
          };
          this.modelStatus.textContent = statusTexts[this.activeModel];
        }

        handleDragOver(e) {
          e.preventDefault();
          this.dropZone.classList.add("drag-over");
        }

        handleDrop(e) {
          e.preventDefault();
          this.dropZone.classList.remove("drag-over");
          const files = e.dataTransfer.files;
          if (files.length > 0 && files[0].type.startsWith("image/")) {
            this.loadImage(files[0]);
          }
        }

        handleFileSelect(e) {
          const file = e.target.files[0];
          if (file && file.type.startsWith("image/")) {
            this.loadImage(file);
          }
        }

        loadImage(file) {
          const reader = new FileReader();
          reader.onload = (e) => {
            this.imageDisplay.src = e.target.result;
            this.imageDisplay.onload = () => {
              this.currentImage = this.imageDisplay;
              this.imageContainer.style.display = "block";
              this.setupCanvas();
              this.status.textContent =
                'Image loaded! Click "Analyze Expression" to detect facial expressions.';
              this.predictions.innerHTML = "";
            };
          };
          reader.readAsDataURL(file);
        }

        async startCamera() {
          try {
            this.stream = await navigator.mediaDevices.getUserMedia({
              video: { facingMode: "user" },
            });
            this.video.srcObject = this.stream;
            this.cameraSection.style.display = "block";
            this.status.textContent =
              'Camera active. Click "Capture Photo" to take a picture.';
          } catch (error) {
            console.error("Camera error:", error);
            this.status.textContent =
              "Camera access denied. Please allow camera access or upload an image.";
          }
        }

        capturePhoto() {
          const canvas = document.createElement("canvas");
          canvas.width = this.video.videoWidth;
          canvas.height = this.video.videoHeight;
          const ctx = canvas.getContext("2d");

          ctx.scale(-1, 1);
          ctx.drawImage(
            this.video,
            -canvas.width,
            0,
            canvas.width,
            canvas.height
          );

          this.imageDisplay.src = canvas.toDataURL();
          this.imageDisplay.onload = () => {
            this.currentImage = this.imageDisplay;
            this.imageContainer.style.display = "block";
            this.setupCanvas();
            this.closeCamera();
            this.status.textContent =
              'Photo captured! Click "Analyze Expression" to detect facial expressions.';
            this.predictions.innerHTML = "";
          };
        }

        closeCamera() {
          if (this.stream) {
            this.stream.getTracks().forEach((track) => track.stop());
            this.stream = null;
          }
          this.cameraSection.style.display = "none";
          this.status.textContent =
            "Camera closed. Upload an image or take another photo to analyze.";
        }

        setupCanvas() {
          this.canvas.width = this.imageDisplay.width;
          this.canvas.height = this.imageDisplay.height;
          this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
        }

        async analyzeExpression() {
          if (!this.currentImage || !this.modelsLoaded) {
            this.status.textContent = this.modelsLoaded
              ? "Please upload an image or take a photo first."
              : "Models are still loading. Please wait.";
            return;
          }

          this.analyzeButton.disabled = true;
          this.status.innerHTML =
            '<div class="loading"><div class="spinner"></div>Analyzing facial expressions...</div>';

          try {
            let detections;

            switch (this.activeModel) {
              case "face-api":
                detections = await this.analyzeFaceApi();
                break;
              case "mobilenet":
                detections = await this.analyzeMobileNet();
                break;
              case "ensemble":
                detections = await this.analyzeEnsemble();
                break;
              default:
                detections = await this.analyzeFaceApi();
            }

            if (!detections || detections.length === 0) {
              this.status.textContent =
                "No faces detected in the image. Please try another image with clear faces.";
              this.analyzeButton.disabled = false;
              return;
            }

            this.drawDetections(detections);
            this.displayDetections(detections);

            this.status.textContent = `Analysis complete! Found ${detections.length} face(s) in the image.`;
          } catch (error) {
            console.error("Analysis error:", error);
            this.status.textContent =
              "Error analyzing image. Please try again with a different image.";
          } finally {
            this.analyzeButton.disabled = false;
          }
        }

        async analyzeFaceApi() {
          if (!this.models.faceApi) {
            throw new Error("Face-API model not loaded");
          }

          const detections = await faceapi
            .detectAllFaces(
              this.currentImage,
              new faceapi.TinyFaceDetectorOptions()
            )
            .withFaceLandmarks()
            .withFaceExpressions()
            .withAgeAndGender();

          return detections;
        }

        async analyzeMobileNet() {
          if (!this.models.mobilenet) {
            throw new Error("MobileNet model not loaded");
          }

          // First detect faces using Face-API
          const faceDetections = await faceapi
            .detectAllFaces(
              this.currentImage,
              new faceapi.TinyFaceDetectorOptions()
            )
            .withFaceLandmarks();

          if (faceDetections.length === 0) {
            return [];
          }

          const results = [];
          for (const detection of faceDetections) {
            // Extract face region
            const faceCanvas = this.extractFaceRegion(detection);

            // Preprocess for MobileNet
            const preprocessed = this.preprocessForMobileNet(faceCanvas);

            // Get predictions
            const predictions = await this.models.mobilenet.predict(
              preprocessed
            );
            const probabilities = await predictions.data();

            // Convert to expression format
            const expressions = this.convertToExpressions(probabilities);

            results.push({
              detection: detection,
              expressions: expressions,
              landmarks: detection.landmarks,
            });
          }

          return results;
        }

        async analyzeEnsemble() {
          if (this.models.ensemble.length === 0) {
            return await this.analyzeFaceApi();
          }

          // Get predictions from all models
          const faceApiResults = await this.analyzeFaceApi();
          const mobileNetResults = await this.analyzeMobileNet();

          // Combine results using weighted average
          const ensembleResults = [];

          for (let i = 0; i < faceApiResults.length; i++) {
            const faceApi = faceApiResults[i];
            const mobileNet = mobileNetResults[i] || faceApi;

            // Weighted ensemble (Face-API: 0.6, MobileNet: 0.4)
            const combinedExpressions = {};
            const expressionNames = [
              "angry",
              "disgusted",
              "fearful",
              "happy",
              "neutral",
              "sad",
              "surprised",
            ];

            for (const expr of expressionNames) {
              const faceApiScore = faceApi.expressions[expr] || 0;
              const mobileNetScore = mobileNet.expressions[expr] || 0;
              combinedExpressions[expr] =
                faceApiScore * 0.6 + mobileNetScore * 0.4;
            }

            ensembleResults.push({
              detection: faceApi.detection,
              expressions: combinedExpressions,
              landmarks: faceApi.landmarks,
              age: faceApi.age,
              gender: faceApi.gender,
              genderProbability: faceApi.genderProbability,
            });
          }

          return ensembleResults;
        }

        extractFaceRegion(detection) {
          const canvas = document.createElement("canvas");
          const ctx = canvas.getContext("2d");
          const box = detection.detection.box;

          canvas.width = box.width;
          canvas.height = box.height;

          ctx.drawImage(
            this.currentImage,
            box.x,
            box.y,
            box.width,
            box.height,
            0,
            0,
            box.width,
            box.height
          );

          return canvas;
        }

        preprocessForMobileNet(canvas) {
          // Resize to 224x224 for MobileNet
          const resized = tf.browser
            .fromPixels(canvas)
            .resizeNearestNeighbor([224, 224])
            .expandDims(0)
            .div(255.0);

          return resized;
        }

        convertToExpressions(probabilities) {
          const expressionNames = [
            "angry",
            "disgusted",
            "fearful",
            "happy",
            "neutral",
            "sad",
            "surprised",
          ];
          const expressions = {};

          for (let i = 0; i < expressionNames.length; i++) {
            expressions[expressionNames[i]] = probabilities[i];
          }

          return expressions;
        }

        drawDetections(detections) {
          this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);

          detections.forEach((detection, index) => {
            const box = detection.detection.box;

            // Draw face box
            this.ctx.strokeStyle = "#4caf50";
            this.ctx.lineWidth = 3;
            this.ctx.strokeRect(box.x, box.y, box.width, box.height);

            // Draw face number and confidence
            this.ctx.fillStyle = "#4caf50";
            this.ctx.font = "bold 16px Arial";
            const confidence = (detection.detection.score * 100).toFixed(0);
            this.ctx.fillText(
              `Face ${index + 1} (${confidence}%)`,
              box.x,
              box.y - 5
            );

            // Draw landmarks if available
            if (detection.landmarks) {
              this.ctx.fillStyle = "#ff4444";
              const landmarks = detection.landmarks.positions;
              landmarks.forEach((point) => {
                this.ctx.beginPath();
                this.ctx.arc(point.x, point.y, 2, 0, 2 * Math.PI);
                this.ctx.fill();
              });
            }
          });
        }

        displayDetections(detections) {
          this.predictions.innerHTML = "";

          detections.forEach((detection, faceIndex) => {
            // Create face info section
            const faceInfo = document.createElement("div");
            faceInfo.className = "face-info";

            let faceInfoHtml = `<h3>Face ${
              faceIndex + 1
            }</h3><div class="face-details">`;
            faceInfoHtml += `<span>Confidence: ${(
              detection.detection.score * 100
            ).toFixed(1)}%</span>`;

            if (detection.age) {
              faceInfoHtml += `<span>Age: ${Math.round(detection.age)}</span>`;
            }

            if (detection.gender) {
              faceInfoHtml += `<span>Gender: ${detection.gender} (${(
                detection.genderProbability * 100
              ).toFixed(1)}%)</span>`;
            }

            faceInfoHtml += `</div>`;
            faceInfo.innerHTML = faceInfoHtml;
            this.predictions.appendChild(faceInfo);

            // Display expressions
            const sortedExpressions = Object.entries(detection.expressions)
              .map(([emotion, confidence]) => ({
                expression: emotion.charAt(0).toUpperCase() + emotion.slice(1),
                emoji: this.getEmoji(emotion),
                confidence: confidence,
              }))
              .sort((a, b) => b.confidence - a.confidence);

            sortedExpressions.forEach((pred, index) => {
              const card = document.createElement("div");
              card.className = "prediction-card";
              if (index === 0) {
                card.classList.add("top-prediction");
              }

              const confidencePercent = (pred.confidence * 100).toFixed(1);

              card.innerHTML = `
                            <div class="expression-emoji">${pred.emoji}</div>
                            <div class="expression-name">${pred.expression}</div>
                            <div class="confidence-bar">
                                <div class="confidence-fill" style="width: ${confidencePercent}%"></div>
                            </div>
                            <div class="confidence-text">${confidencePercent}%</div>
                        `;

              this.predictions.appendChild(card);
            });
          });
        }

        getEmoji(emotion) {
          const emojiMap = {
            happy: "üòä",
            sad: "üò¢",
            angry: "üò†",
            surprised: "üòÆ",
            fearful: "üò®",
            disgusted: "ü§¢",
            neutral: "üòê",
          };
          return emojiMap[emotion.toLowerCase()] || "üòê";
        }
      }

      // Initialize the application
      document.addEventListener("DOMContentLoaded", () => {
        new AdvancedFaceExpressionDetector();
      });
    </script>
  </body>
</html>

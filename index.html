<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Advanced Face Expression Detection</title>
    <!-- Load TensorFlow.js first -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.2.0/dist/tf.min.js"></script>
    <!-- 
      REMOVED: face-api.min.js script tag. 
      It will be loaded dynamically in the script to avoid library conflicts.
    -->
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: "Arial", sans-serif;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        min-height: 100vh;
        display: flex;
        flex-direction: column;
        align-items: center;
        padding: 20px;
        color: white;
      }

      .container {
        max-width: 1200px;
        width: 100%;
        display: flex;
        flex-direction: column;
        align-items: center;
      }

      h1 {
        font-size: 2.5rem;
        margin-bottom: 30px;
        text-align: center;
        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
      }

      .model-info {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        padding: 20px;
        margin-bottom: 20px;
        text-align: center;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
      }

      .upload-section {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 20px;
        padding: 30px;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        margin-bottom: 30px;
        text-align: center;
        width: 100%;
        max-width: 600px;
      }

      .upload-options {
        display: flex;
        gap: 20px;
        justify-content: center;
        margin-bottom: 30px;
        flex-wrap: wrap;
      }

      .upload-button {
        padding: 15px 30px;
        border: none;
        border-radius: 25px;
        background: rgba(255, 255, 255, 0.2);
        color: white;
        font-size: 16px;
        cursor: pointer;
        transition: all 0.3s ease;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.3);
        display: flex;
        align-items: center;
        gap: 10px;
      }

      .upload-button:hover {
        background: rgba(255, 255, 255, 0.3);
        transform: translateY(-2px);
      }

      .upload-button:disabled {
        opacity: 0.5;
        cursor: not-allowed;
      }

      .file-input {
        display: none;
      }

      .camera-section {
        display: none;
        margin-top: 20px;
      }

      .camera-container {
        position: relative;
        margin-bottom: 20px;
      }

      #videoElement {
        width: 400px;
        height: 300px;
        border-radius: 15px;
        transform: scaleX(-1);
      }

      .camera-controls {
        display: flex;
        gap: 15px;
        justify-content: center;
      }

      .image-container {
        position: relative;
        background: rgba(255, 255, 255, 0.1);
        border-radius: 20px;
        padding: 20px;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        margin-bottom: 30px;
        display: none;
      }

      #imageDisplay {
        max-width: 100%;
        max-height: 500px;
        border-radius: 15px;
        box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
      }

      .canvas-overlay {
        position: absolute;
        top: 20px;
        left: 20px;
        pointer-events: none;
        border-radius: 15px;
      }

      .status {
        margin-bottom: 20px;
        padding: 15px 25px;
        border-radius: 15px;
        background: rgba(0, 0, 0, 0.2);
        font-size: 18px;
        text-align: center;
        max-width: 600px;
        width: 100%;
      }

      .predictions {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
        gap: 20px;
        width: 100%;
        max-width: 1000px;
      }

      .prediction-card {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        padding: 20px;
        text-align: center;
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        transition: transform 0.3s ease;
      }

      .prediction-card:hover {
        transform: translateY(-5px);
      }

      .prediction-card.top-prediction {
        border: 2px solid #4caf50;
        box-shadow: 0 5px 20px rgba(76, 175, 80, 0.3);
      }

      .expression-emoji {
        font-size: 3rem;
        margin-bottom: 10px;
      }

      .expression-name {
        font-size: 1.2rem;
        font-weight: bold;
        margin-bottom: 10px;
      }

      .confidence-bar {
        width: 100%;
        height: 20px;
        background: rgba(255, 255, 255, 0.2);
        border-radius: 10px;
        overflow: hidden;
        margin-top: 10px;
      }

      .confidence-fill {
        height: 100%;
        background: linear-gradient(90deg, #4caf50, #8bc34a);
        transition: width 0.5s ease;
        border-radius: 10px;
      }

      .confidence-text {
        font-size: 0.9rem;
        margin-top: 5px;
      }

      .loading {
        text-align: center;
        padding: 20px;
      }

      .spinner {
        border: 4px solid rgba(255, 255, 255, 0.3);
        border-top: 4px solid white;
        border-radius: 50%;
        width: 40px;
        height: 40px;
        animation: spin 1s linear infinite;
        margin: 0 auto 20px;
      }

      @keyframes spin {
        0% {
          transform: rotate(0deg);
        }
        100% {
          transform: rotate(360deg);
        }
      }

      .analyze-button {
        padding: 15px 30px;
        border: none;
        border-radius: 25px;
        background: linear-gradient(45deg, #4caf50, #8bc34a);
        color: white;
        font-size: 18px;
        font-weight: bold;
        cursor: pointer;
        transition: all 0.3s ease;
        margin-top: 20px;
        box-shadow: 0 5px 15px rgba(76, 175, 80, 0.3);
      }

      .analyze-button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(76, 175, 80, 0.4);
      }

      .analyze-button:disabled {
        opacity: 0.5;
        cursor: not-allowed;
        transform: none;
      }

      .drop-zone {
        border: 2px dashed rgba(255, 255, 255, 0.3);
        border-radius: 15px;
        padding: 40px;
        text-align: center;
        transition: all 0.3s ease;
        cursor: pointer;
        margin-bottom: 20px;
      }

      .drop-zone:hover,
      .drop-zone.drag-over {
        border-color: #4caf50;
        background: rgba(76, 175, 80, 0.1);
      }

      .drop-zone-text {
        font-size: 1.1rem;
        margin-bottom: 15px;
        opacity: 0.8;
      }

      .face-info {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 15px;
        padding: 15px;
        margin-bottom: 20px;
        text-align: center;
      }

      .face-details {
        display: flex;
        justify-content: space-around;
        margin-top: 10px;
        font-size: 0.9rem;
      }

      .model-switch {
        display: flex;
        gap: 10px;
        margin-bottom: 15px;
        justify-content: center;
      }

      .model-button {
        padding: 8px 16px;
        border: none;
        border-radius: 20px;
        background: rgba(255, 255, 255, 0.2);
        color: white;
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 14px;
      }

      .model-button.active {
        background: #4caf50;
      }

      @media (max-width: 768px) {
        #videoElement {
          width: 300px;
          height: 225px;
        }
        h1 {
          font-size: 2rem;
        }
        .predictions {
          grid-template-columns: 1fr;
        }
        .upload-options {
          flex-direction: column;
          align-items: center;
        }
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>üé≠ Advanced Face Expression Detection</h1>

      <div class="model-info">
        <div class="model-switch">
          <button class="model-button active" data-model="face-api">
            Face-API.js
          </button>
          <button class="model-button" data-model="mobilenet">MobileNet</button>
          <button class="model-button" data-model="ensemble">Ensemble</button>
        </div>
        <div id="modelStatus">
          Using Face-API.js with Expression Recognition
        </div>
      </div>

      <div class="status" id="status">Loading AI models... Please wait.</div>

      <div class="upload-section">
        <div class="drop-zone" id="dropZone">
          <div class="drop-zone-text">
            Drag & drop an image here, or click to upload
          </div>
          <div style="font-size: 2rem; margin-bottom: 15px">üì∏</div>
        </div>

        <div class="upload-options">
          <button class="upload-button" id="uploadButton">
            üìÅ Upload Image
          </button>
          <button class="upload-button" id="cameraButton">üì∑ Take Photo</button>
        </div>

        <input type="file" id="fileInput" class="file-input" accept="image/*" />

        <div class="camera-section" id="cameraSection">
          <div class="camera-container">
            <video id="videoElement" autoplay muted playsinline></video>
          </div>
          <div class="camera-controls">
            <button class="upload-button" id="captureButton">
              üì∑ Capture Photo
            </button>
            <button class="upload-button" id="closeCameraButton">
              ‚ùå Close Camera
            </button>
          </div>
        </div>
      </div>

      <div class="image-container" id="imageContainer">
        <img id="imageDisplay" alt="Uploaded image" />
        <canvas id="overlay" class="canvas-overlay"></canvas>
        <div style="text-align: center">
          <button class="analyze-button" id="analyzeButton">
            üîç Analyze Expression
          </button>
        </div>
      </div>

      <div class="predictions" id="predictions"></div>
    </div>

    <script>
      class AdvancedFaceExpressionDetector {
        constructor() {
          this.imageDisplay = document.getElementById("imageDisplay");
          this.canvas = document.getElementById("overlay");
          this.ctx = this.canvas.getContext("2d");
          this.status = document.getElementById("status");
          this.predictions = document.getElementById("predictions");
          this.imageContainer = document.getElementById("imageContainer");
          this.analyzeButton = document.getElementById("analyzeButton");
          this.modelStatus = document.getElementById("modelStatus");

          this.uploadButton = document.getElementById("uploadButton");
          this.cameraButton = document.getElementById("cameraButton");
          this.fileInput = document.getElementById("fileInput");
          this.dropZone = document.getElementById("dropZone");

          this.cameraSection = document.getElementById("cameraSection");
          this.video = document.getElementById("videoElement");
          this.captureButton = document.getElementById("captureButton");
          this.closeCameraButton = document.getElementById("closeCameraButton");

          this.currentImage = null;
          this.stream = null;
          this.modelsLoaded = false;
          this.activeModel = "face-api";

          // Model storage
          this.models = {
            faceApi: null,
            mobilenet: null,
            ensemble: [],
          };

          this.expressions = [
            { name: "Happy", emoji: "üòä" },
            { name: "Sad", emoji: "üò¢" },
            { name: "Angry", emoji: "üò†" },
            { name: "Surprised", emoji: "üòÆ" },
            { name: "Fearful", emoji: "üò®" },
            { name: "Disgusted", emoji: "ü§¢" },
            { name: "Neutral", emoji: "üòê" },
          ];

          this.init();
        }

        async init() {
          try {
            await this.loadAllModels();
            this.setupEventListeners();
            this.status.textContent =
              "Ready! All models loaded successfully. Upload an image to analyze expressions.";
          } catch (error) {
            console.error("Initialization error:", error);
            this.status.textContent =
              "Error loading models. Please refresh the page.";
          }
        }

        async loadAllModels() {
          this.status.textContent =
            "Loading multiple AI models for best accuracy...";

          try {
            // Load all models in parallel
            await Promise.all([
              this.loadFaceApiModels(),
              this.loadMobileNetModel(),
              this.loadEnsembleModels(),
            ]);

            this.modelsLoaded = true;
            this.status.textContent = "All models loaded successfully!";
          } catch (error) {
            console.error("Model loading error:", error);
            this.status.textContent =
              `Failed to load all models: ${error.message}. Some features may be limited.`;
            // Attempt to recover with just FaceAPI if it's loaded
            if (this.models.faceApi) {
                this.modelsLoaded = true;
                this.status.textContent = "Primary model loaded. Some advanced features may be unavailable."
            }
          }
        }

        async loadFaceApiModels() {
          try {
            this.status.textContent = "Loading Face-API models...";
            // FIX: Dynamically import the esm module to avoid tfjs conflicts
            const faceapi = await import(
              "https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.13/dist/face-api.esm.js"
            );
            this.models.faceApi = faceapi;

            // Explicitly set backend
            await this.models.faceApi.tf.setBackend("webgl");
            await this.models.faceApi.tf.ready();

            const MODEL_URL =
              "https://cdn.jsdelivr.net/npm/@vladmandic/face-api@latest/model/";

            await Promise.all([
              this.models.faceApi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
              this.models.faceApi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
              this.models.faceApi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
              this.models.faceApi.nets.faceExpressionNet.loadFromUri(MODEL_URL),
              this.models.faceApi.nets.ageGenderNet.loadFromUri(MODEL_URL),
            ]);

            console.log("Face-API models loaded successfully");
          } catch (error) {
            console.error("Face-API loading failed:", error);
            throw error;
          }
        }

        async loadMobileNetModel() {
          try {
            this.status.textContent = "Loading MobileNet emotion model...";

            // FIX: Load the base model first
            const mobilenetBase = await tf.loadLayersModel(
              "https://tfhub.dev/google/tfjs-model/imagenet/mobilenet_v2_100_224/feature_vector/3/default/1",
              { fromTFHub: true } // FIX: This is required for TF Hub models
            );

            // FIX: Create the full model using the loaded base model
            const model = this.createMobileNetEmotionModel(mobilenetBase);
            this.models.mobilenet = model;

            console.log("MobileNet model created successfully");
          } catch (error) {
            console.error("MobileNet loading failed:", error);
            this.models.mobilenet = null; // Ensure it's null on failure
            throw error; // Propagate error to stop initialization if needed
          }
        }

        createMobileNetEmotionModel(baseModel) {
          // FIX: The base model is used as the first layer of a new sequential model.
          // The baseModel will output a 1280-element feature vector.
          const model = tf.sequential({
            layers: [
              baseModel, // The pre-trained MobileNet base
              tf.layers.dense({
                // The inputShape is now inferred from the baseModel's output
                units: 256,
                activation: "relu",
                kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }),
              }),
              tf.layers.dropout({ rate: 0.5 }),
              tf.layers.dense({
                units: 128,
                activation: "relu",
                kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }),
              }),
              tf.layers.dropout({ rate: 0.3 }),
              tf.layers.dense({
                units: 7,
                activation: "softmax",
              }),
            ],
          });

          // Model is not trained here, just assembled. We use its pre-trained weights.
          // A compile step is good practice even for inference.
          model.compile({
            optimizer: tf.train.adam(0.001),
            loss: "categoricalCrossentropy",
            metrics: ["accuracy"],
          });

          return model;
        }

        async loadEnsembleModels() {
          try {
            this.status.textContent = "Loading ensemble models...";
            // These models are created synchronously.
            const models = [
              this.createCNNModel(),
              this.createResNetModel(),
              this.createAttentionModel(),
            ];

            this.models.ensemble = models;
            console.log("Ensemble models loaded successfully");
          } catch (error) {
            console.error("Ensemble loading failed:", error);
            this.models.ensemble = [];
            throw error; // Propagate error
          }
        }
        
        // FIX: Removed unnecessary `async`
        createCNNModel() {
          const model = tf.sequential({
            layers: [
              tf.layers.conv2d({
                inputShape: [48, 48, 1], filters: 32, kernelSize: 3, activation: "relu", padding: "same"
              }),
              tf.layers.batchNormalization(),
              tf.layers.maxPooling2d({ poolSize: 2 }),
              tf.layers.dropout({ rate: 0.25 }),
              tf.layers.conv2d({ filters: 64, kernelSize: 3, activation: "relu", padding: "same" }),
              tf.layers.batchNormalization(),
              tf.layers.maxPooling2d({ poolSize: 2 }),
              tf.layers.dropout({ rate: 0.25 }),
              tf.layers.conv2d({ filters: 128, kernelSize: 3, activation: "relu", padding: "same" }),
              tf.layers.batchNormalization(),
              tf.layers.maxPooling2d({ poolSize: 2 }),
              tf.layers.dropout({ rate: 0.25 }),
              tf.layers.flatten(),
              tf.layers.dense({ units: 512, activation: "relu", kernelRegularizer: tf.regularizers.l2({ l2: 0.001 }) }),
              tf.layers.dropout({ rate: 0.5 }),
              tf.layers.dense({ units: 7, activation: "softmax" }),
            ],
          });
          model.compile({ optimizer: tf.train.adam(0.001), loss: "categoricalCrossentropy", metrics: ["accuracy"] });
          return model;
        }

        // FIX: Removed unnecessary `async`
        createResNetModel() {
          const input = tf.input({ shape: [48, 48, 1] });
          let x = tf.layers.conv2d({ filters: 64, kernelSize: 7, strides: 2, padding: "same", activation: "relu" }).apply(input);
          x = tf.layers.batchNormalization().apply(x);
          x = tf.layers.maxPooling2d({ poolSize: 3, strides: 2, padding: "same" }).apply(x);
          x = this.residualBlock(x, 64);
          x = this.residualBlock(x, 128, 2);
          x = this.residualBlock(x, 256, 2);
          x = tf.layers.globalAveragePooling2d({}).apply(x);
          x = tf.layers.dense({ units: 7, activation: "softmax" }).apply(x);
          const model = tf.model({ inputs: input, outputs: x });
          model.compile({ optimizer: tf.train.adam(0.001), loss: "categoricalCrossentropy", metrics: ["accuracy"] });
          return model;
        }

        residualBlock(input, filters, strides = 1) {
          const shortcut = input;
          let x = tf.layers.conv2d({ filters, kernelSize: 3, strides, padding: "same" }).apply(input);
          x = tf.layers.batchNormalization().apply(x);
          x = tf.layers.activation({ activation: "relu" }).apply(x);
          x = tf.layers.conv2d({ filters, kernelSize: 3, strides: 1, padding: "same" }).apply(x);
          x = tf.layers.batchNormalization().apply(x);
          let adjustedShortcut = shortcut;
          if (strides !== 1 || input.shape[3] !== filters) {
            adjustedShortcut = tf.layers.conv2d({ filters, kernelSize: 1, strides, padding: "same" }).apply(shortcut);
            adjustedShortcut = tf.layers.batchNormalization().apply(adjustedShortcut);
          }
          x = tf.layers.add().apply([x, adjustedShortcut]);
          x = tf.layers.activation({ activation: "relu" }).apply(x);
          return x;
        }
        
        // FIX: Removed unnecessary `async`
        createAttentionModel() {
          const model = tf.sequential({
            layers: [
              tf.layers.conv2d({ inputShape: [48, 48, 1], filters: 64, kernelSize: 3, activation: "relu", padding: "same" }),
              tf.layers.batchNormalization(),
              tf.layers.maxPooling2d({ poolSize: 2 }),
              tf.layers.conv2d({ filters: 128, kernelSize: 3, activation: "relu", padding: "same" }),
              tf.layers.batchNormalization(),
              tf.layers.maxPooling2d({ poolSize: 2 }),
              tf.layers.conv2d({ filters: 256, kernelSize: 3, activation: "relu", padding: "same" }),
              tf.layers.batchNormalization(),
              tf.layers.globalAveragePooling2d(),
              tf.layers.dense({ units: 128, activation: "relu" }),
              tf.layers.dropout({ rate: 0.5 }),
              tf.layers.dense({ units: 7, activation: "softmax" }),
            ],
          });
          model.compile({ optimizer: tf.train.adam(0.001), loss: "categoricalCrossentropy", metrics: ["accuracy"] });
          return model;
        }

        setupEventListeners() {
          document.querySelectorAll(".model-button").forEach((button) => {
            button.addEventListener("click", (e) => {
              document.querySelectorAll(".model-button").forEach((b) => b.classList.remove("active"));
              button.classList.add("active");
              this.activeModel = button.dataset.model;
              this.updateModelStatus();
            });
          });
          this.uploadButton.addEventListener("click", () => this.fileInput.click());
          this.fileInput.addEventListener("change", (e) => this.handleFileSelect(e));
          this.dropZone.addEventListener("click", () => this.fileInput.click());
          this.dropZone.addEventListener("dragover", (e) => this.handleDragOver(e));
          this.dropZone.addEventListener("drop", (e) => this.handleDrop(e));
          this.dropZone.addEventListener("dragleave", () => this.dropZone.classList.remove("drag-over"));
          this.cameraButton.addEventListener("click", () => this.startCamera());
          this.captureButton.addEventListener("click", () => this.capturePhoto());
          this.closeCameraButton.addEventListener("click", () => this.closeCamera());
          this.analyzeButton.addEventListener("click", () => this.analyzeExpression());
        }

        updateModelStatus() {
          const statusTexts = {
            "face-api": "Using Face-API.js with Expression Recognition (High Accuracy)",
            mobilenet: "Using MobileNet-based Emotion Model (Fast & Accurate)",
            ensemble: "Using Ensemble of Multiple Models (Highest Accuracy)",
          };
          this.modelStatus.textContent = statusTexts[this.activeModel];
        }

        handleDragOver(e) {
          e.preventDefault();
          this.dropZone.classList.add("drag-over");
        }

        handleDrop(e) {
          e.preventDefault();
          this.dropZone.classList.remove("drag-over");
          const files = e.dataTransfer.files;
          if (files.length > 0 && files[0].type.startsWith("image/")) {
            this.loadImage(files[0]);
          }
        }

        handleFileSelect(e) {
          const file = e.target.files[0];
          if (file && file.type.startsWith("image/")) {
            this.loadImage(file);
          }
        }

        loadImage(file) {
          const reader = new FileReader();
          reader.onload = (e) => {
            this.imageDisplay.src = e.target.result;
            this.imageDisplay.onload = () => {
              this.currentImage = this.imageDisplay;
              this.imageContainer.style.display = "block";
              this.setupCanvas();
              this.status.textContent = 'Image loaded! Click "Analyze Expression" to detect facial expressions.';
              this.predictions.innerHTML = "";
            };
          };
          reader.readAsDataURL(file);
        }

        async startCamera() {
          try {
            this.stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "user" } });
            this.video.srcObject = this.stream;
            this.cameraSection.style.display = "block";
            this.status.textContent = 'Camera active. Click "Capture Photo" to take a picture.';
          } catch (error) {
            console.error("Camera error:", error);
            this.status.textContent = "Camera access denied. Please allow camera access or upload an image.";
          }
        }

        capturePhoto() {
          const canvas = document.createElement("canvas");
          canvas.width = this.video.videoWidth;
          canvas.height = this.video.videoHeight;
          const ctx = canvas.getContext("2d");
          ctx.scale(-1, 1);
          ctx.drawImage(this.video, -canvas.width, 0, canvas.width, canvas.height);
          this.imageDisplay.src = canvas.toDataURL();
          this.imageDisplay.onload = () => {
            this.currentImage = this.imageDisplay;
            this.imageContainer.style.display = "block";
            this.setupCanvas();
            this.closeCamera();
            this.status.textContent = 'Photo captured! Click "Analyze Expression" to detect facial expressions.';
            this.predictions.innerHTML = "";
          };
        }

        closeCamera() {
          if (this.stream) {
            this.stream.getTracks().forEach((track) => track.stop());
            this.stream = null;
          }
          this.cameraSection.style.display = "none";
          this.status.textContent = "Camera closed. Upload an image or take another photo to analyze.";
        }

        setupCanvas() {
          this.canvas.width = this.imageDisplay.width;
          this.canvas.height = this.imageDisplay.height;
          this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
        }

        async analyzeExpression() {
          if (!this.currentImage || !this.modelsLoaded) {
            this.status.textContent = this.modelsLoaded ? "Please upload an image or take a photo first." : "Models are still loading. Please wait.";
            return;
          }
          this.analyzeButton.disabled = true;
          this.status.innerHTML = '<div class="loading"><div class="spinner"></div>Analyzing facial expressions...</div>';
          try {
            let detections;
            switch (this.activeModel) {
              case "face-api":
                detections = await this.analyzeFaceApi();
                break;
              case "mobilenet":
                detections = await this.analyzeMobileNet();
                break;
              case "ensemble":
                detections = await this.analyzeEnsemble();
                break;
              default:
                detections = await this.analyzeFaceApi();
            }
            if (!detections || detections.length === 0) {
              this.status.textContent = "No faces detected in the image. Please try another image with clear faces.";
              this.analyzeButton.disabled = false;
              return;
            }
            this.drawDetections(detections);
            this.displayDetections(detections);
            this.status.textContent = `Analysis complete! Found ${detections.length} face(s) in the image.`;
          } catch (error) {
            console.error("Analysis error:", error);
            this.status.textContent = `Error analyzing image: ${error.message}. Please try again.`;
          } finally {
            this.analyzeButton.disabled = false;
          }
        }

        async analyzeFaceApi() {
          if (!this.models.faceApi) throw new Error("Face-API model not loaded");
          const faceapi = this.models.faceApi;
          return await faceapi
            .detectAllFaces(this.currentImage, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withFaceExpressions()
            .withAgeAndGender();
        }

        async analyzeMobileNet() {
          if (!this.models.mobilenet) throw new Error("MobileNet model not loaded");
          if (!this.models.faceApi) throw new Error("Face-API model not loaded for face detection");
          const faceapi = this.models.faceApi;

          // FIX: Detect faces with all attributes to ensure compatibility with ensemble mode
          const faceDetections = await faceapi
            .detectAllFaces(this.currentImage, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withAgeAndGender();
            
          if (faceDetections.length === 0) return [];
          const results = [];
          for (const detection of faceDetections) {
            const faceCanvas = this.extractFaceRegion(detection);
            const preprocessed = this.preprocessForMobileNet(faceCanvas);
            const predictions = this.models.mobilenet.predict(preprocessed);
            const probabilities = await predictions.data();
            tf.dispose([preprocessed, predictions]); // Clean up tensors
            const expressions = this.convertToExpressions(probabilities);

            // FIX: Construct a result object that mimics the face-api result structure
            results.push({
              detection: detection.detection,
              expressions: expressions,
              landmarks: detection.landmarks,
              age: detection.age,
              gender: detection.gender,
              genderProbability: detection.genderProbability,
            });
          }
          return results;
        }

        async analyzeEnsemble() {
          // Fallback if models failed to load
          if (!this.models.faceApi || !this.models.mobilenet) {
            console.warn("Ensemble mode requires FaceAPI and MobileNet. Falling back to FaceAPI.");
            return await this.analyzeFaceApi();
          }

          const [faceApiResults, mobileNetResults] = await Promise.all([
             this.analyzeFaceApi(),
             this.analyzeMobileNet()
          ]);
          
          if (faceApiResults.length === 0) return [];

          const ensembleResults = [];
          for (let i = 0; i < faceApiResults.length; i++) {
            const faceApi = faceApiResults[i];
            const mobileNet = mobileNetResults.find(res => {
                 const dx = res.detection.box.x - faceApi.detection.box.x;
                 const dy = res.detection.box.y - faceApi.detection.box.y;
                 return Math.sqrt(dx*dx + dy*dy) < 10; // Simple matching
            }) || faceApi;

            const combinedExpressions = {};
            const expressionNames = ["angry", "disgusted", "fearful", "happy", "neutral", "sad", "surprised"];
            for (const expr of expressionNames) {
              const faceApiScore = faceApi.expressions[expr] || 0;
              const mobileNetScore = mobileNet.expressions[expr] || 0;
              // Weighted average: 60% FaceAPI, 40% MobileNet
              combinedExpressions[expr] = faceApiScore * 0.6 + mobileNetScore * 0.4;
            }
            ensembleResults.push({
              detection: faceApi.detection,
              expressions: combinedExpressions,
              landmarks: faceApi.landmarks,
              age: faceApi.age,
              gender: faceApi.gender,
              genderProbability: faceApi.genderProbability,
            });
          }
          return ensembleResults;
        }

        extractFaceRegion(detection) {
          const canvas = document.createElement("canvas");
          const ctx = canvas.getContext("2d");
          const { box } = detection.detection;
          canvas.width = box.width;
          canvas.height = box.height;
          ctx.drawImage(this.currentImage, box.x, box.y, box.width, box.height, 0, 0, box.width, box.height);
          return canvas;
        }

        // ADDED: Missing pre-processing function for MobileNet
        preprocessForMobileNet(canvas) {
          return tf.tidy(() => {
            const tensor = tf.browser.fromPixels(canvas)
              .resizeNearestNeighbor([224, 224])
              .toFloat()
              .div(255.0) // Normalize to [0, 1] as expected by this TF Hub model
              .expandDims(0); // Add batch dimension
            return tensor;
          });
        }

        convertToExpressions(probabilities) {
          const expressionNames = ["angry", "disgusted", "fearful", "happy", "neutral", "sad", "surprised"];
          const expressions = {};
          for (let i = 0; i < expressionNames.length; i++) {
            expressions[expressionNames[i]] = probabilities[i];
          }
          return expressions;
        }

        drawDetections(detections) {
          this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
          const faceapi = this.models.faceApi;
          detections.forEach((detection) => {
             const box = detection.detection.box;
             this.ctx.strokeStyle = "#4caf50";
             this.ctx.lineWidth = 3;
             this.ctx.strokeRect(box.x, box.y, box.width, box.height);
             
             // Use Face-API's draw methods for convenience if you want more details
             // For example: faceapi.draw.drawFaceLandmarks(this.canvas, detection);
          });
        }

        displayDetections(detections) {
          this.predictions.innerHTML = "";

          detections.forEach((detection, faceIndex) => {
            const faceInfo = document.createElement("div");
            faceInfo.className = "face-info";
            let faceInfoHtml = `<h3>Face ${faceIndex + 1}</h3><div class="face-details">`;
            faceInfoHtml += `<span>Confidence: ${(detection.detection.score * 100).toFixed(1)}%</span>`;
            if (detection.age) faceInfoHtml += `<span>Age: ${Math.round(detection.age)}</span>`;
            if (detection.gender) faceInfoHtml += `<span>Gender: ${detection.gender} (${(detection.genderProbability * 100).toFixed(1)}%)</span>`;
            faceInfoHtml += `</div>`;
            faceInfo.innerHTML = faceInfoHtml;
            this.predictions.appendChild(faceInfo);

            const sortedExpressions = Object.entries(detection.expressions)
              .map(([emotion, confidence]) => ({
                expression: emotion.charAt(0).toUpperCase() + emotion.slice(1),
                emoji: this.getEmoji(emotion),
                confidence: confidence,
              }))
              .sort((a, b) => b.confidence - a.confidence);
            
            // NEW: Log accuracy (confidence) to the console
            const topPrediction = sortedExpressions[0];
            console.log(`%cFace ${faceIndex + 1}: Top Prediction`, 'font-weight: bold; color: #4caf50;',
             `${topPrediction.expression} (Confidence: ${(topPrediction.confidence * 100).toFixed(1)}%)`);
            console.log(`Face ${faceIndex + 1} - All Expression Probabilities:`, detection.expressions);

            sortedExpressions.forEach((pred, index) => {
              const card = document.createElement("div");
              card.className = "prediction-card";
              if (index === 0) card.classList.add("top-prediction");
              const confidencePercent = (pred.confidence * 100).toFixed(1);
              card.innerHTML = `
                <div class="expression-emoji">${pred.emoji}</div>
                <div class="expression-name">${pred.expression}</div>
                <div class="confidence-bar">
                    <div class="confidence-fill" style="width: ${confidencePercent}%"></div>
                </div>
                <div class="confidence-text">${confidencePercent}%</div>`;
              this.predictions.appendChild(card);
            });
          });
        }

        getEmoji(emotion) {
          const emojiMap = { happy: "üòä", sad: "üò¢", angry: "üò†", surprised: "üòÆ", fearful: "üò®", disgusted: "ü§¢", neutral: "üòê" };
          return emojiMap[emotion.toLowerCase()] || "üòê";
        }
      }

      document.addEventListener("DOMContentLoaded", () => {
        new AdvancedFaceExpressionDetector();
      });
    </script>
  </body>
</html>